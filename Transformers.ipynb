{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Guide to Transformers in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1: Understanding Transformers\n",
    "\n",
    "#### 1.1 Importance of Transformers\n",
    "\n",
    "Transformers represent a significant advancement in NLP, addressing the limitations of previous models and enabling the development of state-of-the-art models. They are particularly important for generative AI and large language models (LLMs) like BERT and GPT. For instance, OpenAI's ChatGPT, which uses GPT-4, is based on Transformer architecture and is trained with vast amounts of data.\n",
    "\n",
    "##### 1.1.1 Overview of Main Topics\n",
    "- Recurrent Neural Networks (RNN)\n",
    "- Long Short-Term Memory (LSTM)\n",
    "- Gated Recurrent Units (GRU)\n",
    "- Encoder-Decoder Architecture\n",
    "  - Sequence-to-sequence learning\n",
    "  - Attention mechanism\n",
    "\n",
    "#### 1.2 Detailed Architecture\n",
    "\n",
    "The architecture of Transformers includes several key components:\n",
    "- **Encoder and Decoder**: The encoder creates a representation from the input, while the decoder generates the output sequence based on this representation and previously generated tokens.\n",
    "- **Self-Attention Module**: This involves query, key, and value pairs, allowing all words in a sentence to be sent in parallel to the encoder for further processing.\n",
    "- **Positional Encoding**: Ensures the position of each word is taken into account, maintaining the context and meaning of the sentence.\n",
    "- **Multi-Head Attention**: Combines all these components to enhance the model's understanding and processing capabilities.\n",
    "\n",
    "#### 1.3 Sequence-to-Sequence Tasks\n",
    "\n",
    "Transformers are particularly effective for sequence-to-sequence tasks, such as language translation. For example, converting text from English to French involves many-to-many sequence-to-sequence tasks. As the length of the sentences increases, Transformers can handle the complexity and maintain accuracy.\n",
    "\n",
    "#### 1.4 Encoder-Decoder Architecture\n",
    "\n",
    "In the encoder-decoder architecture:\n",
    "- **LSTM**: Used to process entire sentences.\n",
    "- **Words**: Given based on time stamps, converted into vectors using an embedding layer, and then passed to the LSTM.\n",
    "- **Context Vector**: Generated by the LSTM and provided to the next decoder layer for making predictions.\n",
    "- **Challenges**: The context was often insufficient for longer sentences, leading to decreased accuracy.\n",
    "\n",
    "#### 1.5 Attention Mechanism\n",
    "\n",
    "To address the limitations of the encoder-decoder architecture, the attention mechanism was introduced. This mechanism allows for the creation of additional context, improving the accuracy of predictions for longer sentences. Despite its advantages, the attention mechanism still faced scalability issues, as it processed words sequentially based on time stamps.\n",
    "\n",
    "#### 1.6 Scalability and Transfer Learning\n",
    "\n",
    "One of the standout features of Transformers is their scalability. As the size of the dataset increases, Transformers continue to perform exceptionally well, producing models that are at the forefront of NLP research. This scalability is further enhanced by transfer learning. Pre-trained models like BERT and GPT can be fine-tuned for specific tasks without the need to train from scratch, saving time and computational resources.\n",
    "\n",
    "#### 1.7 Application in Multimodal Tasks\n",
    "\n",
    "Transformers are not limited to NLP tasks. They have proven to be highly effective in multimodal tasks that involve both text and images. For instance, OpenAI's DALL-E generates images based on textual descriptions, showcasing the versatility of Transformers. This capability is made possible by the same underlying architecture that powers NLP applications.\n",
    "\n",
    "#### 1.8 Self-Attention Mechanism\n",
    "\n",
    "The self-attention mechanism is central to the functionality of Transformers. It enables all words in a sentence to be sent in parallel to the encoder for further processing. This parallel execution is crucial for handling large datasets efficiently and effectively. The ability to process words simultaneously makes the model scalable and capable of producing state-of-the-art results.\n",
    "\n",
    "##### 1.8.1 Importance of Self-Attention\n",
    "\n",
    "The self-attention mechanism is crucial for the accuracy of Transformers. By capturing contextual relationships between words, it enhances the model's ability to understand and generate text. This makes Transformers particularly effective for a wide range of applications, from NLP to generative AI.\n",
    "\n",
    "##### 1.8.2 Addressing Contextual Embeddings\n",
    "\n",
    "A major limitation of previous models, such as encoder-decoder architectures, was the lack of contextual embeddings. Transformers address this issue through the self-attention mechanism, which creates contextual embeddings that capture the relationships between words in a sentence. This results in more accurate and meaningful representations of text.\n",
    "\n",
    "##### 1.8.3 Example of Contextual Embeddings\n",
    "\n",
    "Consider the sentence: \"My name is Alex and I play chess.\" In this example, the embedding layer generates vectors for each word. However, contextual vectors should reflect the relationships between words, such as the connection between \"Alex\" and \"I.\" The self-attention mechanism ensures that these relationships are captured, leading to more accurate embeddings.\n",
    "\n",
    "#### 1.9 Summary\n",
    "\n",
    "Transformers have revolutionized the field of artificial intelligence by enabling parallel processing of words and creating contextual embeddings. Their scalability and versatility make them suitable for various tasks, including NLP, multimodal applications, and generative AI. The self-attention module and positional encoding are key components that contribute to the success of Transformers, addressing the limitations of previous models and paving the way for future advancements in AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2: Transformers Architecture in Details\n",
    "\n",
    "#### 2.1 Introduction\n",
    "In the Transformer architecture, the order from bottom to top is as follows:\n",
    "\n",
    "1. **Positional Encoding**: This is applied first to give the model information about the position of each word in the sequence.\n",
    "2. **Self-Attention Layer**: After positional encoding, the input goes through the self-attention layer, which converts words into contextual vectors.\n",
    "3. **Feed-Forward Neural Network**: Finally, the output from the self-attention layer is processed by the feed-forward neural network.\n",
    "\n",
    "\n",
    "\n",
    "<style>\n",
    "    .center {\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "        width: 350px; /* Adjust the width as needed */\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<img src=\"Transformer.PNG\" alt=\"Description\" class=\"center\" width=\"350\">\n",
    "\n",
    "\n",
    "\n",
    "#### 2.2 Basic Transformer Architecture\n",
    "The basic Transformer architecture can be understood through a sequence-to-sequence task, such as translating an English sentence into French. The input is an English sentence, and the output is its French translation. We'll focus on the components inside this block diagram.\n",
    "\n",
    "#### 2.3 Transformer Architecture\n",
    "The Transformer architecture features an encoder-decoder structure with multiple encoders and decoders:\n",
    "- **Encoders**: The text input passes through these encoders sequentially.\n",
    "- **Decoders**: The output is generated after passing through multiple decoders.\n",
    "\n",
    "This setup is based on the research paper \"Attention is All You Need,\" which discusses:\n",
    "- Positional encoding\n",
    "- Self-attention\n",
    "- Multi-head attention\n",
    "- Feed-forward networks\n",
    "\n",
    "#### 2.4 Encoder and Decoder Architecture\n",
    "The Transformer model processes the input sentence through the stack of encoders, generating a set of encodings. These encodings are then passed to the decoders, which generate the output sentence. The use of self-attention and multi-head attention allows the model to capture complex dependencies between words, making it highly effective for tasks like translation.\n",
    "\n",
    "#### 2.5 Positional Encoding\n",
    "Transformers do not have a built-in sense of the order of words. Positional encoding is used to give the model information about the position of each word in the sequence. This is achieved by adding sine and cosine functions of different frequencies to the input embeddings.\n",
    "\n",
    "#### 2.6 Self-Attention Layer\n",
    "The self-attention layer converts words into vectors and then into contextual vectors, considering the context of different words:\n",
    "- **Vector Conversion**: Words are converted into vectors.\n",
    "- **Contextual Vectors**: These vectors consider the context of other words.\n",
    "\n",
    "These vectors are processed by the feed-forward neural network and passed to the next encoder. This process repeats through multiple encoders.\n",
    "\n",
    "#### 2.7 Multi-Head Attention\n",
    "Multi-head attention allows the model to focus on different parts of the input sequence simultaneously. It involves running multiple self-attention mechanisms in parallel and then concatenating their outputs.\n",
    "\n",
    "#### 2.8 Feed-Forward Networks\n",
    "Each encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This network consists of two linear transformations with a ReLU activation in between.\n",
    "\n",
    "#### 2.9 Putting It All Together\n",
    "For example, the encoder takes the input \"how are you\" and translates it into French using the decoder. Inside the encoder:\n",
    "- **Self-Attention Layer**: Converts words into vectors.\n",
    "- **Feed-Forward Neural Network Layer**: Processes these vectors.\n",
    "\n",
    "The Transformer model processes the input sentence through the stack of encoders, generating a set of encodings. These encodings are then passed to the decoders, which generate the output sentence. The use of self-attention and multi-head attention allows the model to capture complex dependencies between words, making it highly effective for tasks like translation.\n",
    "\n",
    "#### 2.10 Additional Details\n",
    "- **Parallel Processing**: All words are processed in parallel, enhancing scalability.\n",
    "- **Contextual Accuracy**: Improves accuracy for longer sentences by considering the context of other words.\n",
    "- **Scalability**: The architecture allows for efficient processing of large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3: Self-Attention Mechanism\n",
    "\n",
    "Let's dive deeper into the self-attention layer to understand how it works by focusing extensively on self-attention, including how the entire self-attention layer functions and the mathematical intuition behind it. This will help in understanding how contextual embeddings are created.\n",
    "\n",
    "#### 3.1. Key Points on Self-Attention Layer\n",
    "\n",
    "The idea behind self-attention is to weigh the importance of different tokens in the input sequence relative to each other. Instead of keeping embedding vectors fixed, they are transformed into contextual embedding vectors based on the importance of all the other words. This is crucial for applications like language translation and text summarization.\n",
    "\n",
    "1. **Importance Weighing**: Self-attention assigns different weights to different tokens in the input sequence.\n",
    "2. **Contextual Embeddings**: Embedding vectors are adjusted to reflect the importance of other words in the sequence.\n",
    "3. **Applications**: This mechanism is essential for tasks such as language translation and text summarization.\n",
    "\n",
    "#### 3.2. Illustration of Self-Attention\n",
    "\n",
    "To illustrate, consider the sentence \"the cat sat.\" Each word is converted into vectors, for example:\n",
    "- \"the\" as $$[1, 0, 0]$$\n",
    "- \"cat\" as $$[0, 1, 0]$$\n",
    "- \"sat\" as $$[0, 0, 1]$$\n",
    "\n",
    "Passing these through the self-attention layer results in new vectors, which are the contextual embeddings. These embeddings take into account the importance of different tokens in the input sequence.\n",
    "\n",
    "Self-attention, also known as scaled dot-product attention, is a mechanism in the Transformer architecture that allows the model to weigh the importance of different tokens in the input sequence. The process involves creating three important vectors: queries, keys, and values. These vectors help in determining the importance of other tokens in the context of the current token.\n",
    "\n",
    "#### 3.2.1. Queries, Keys, and Values\n",
    "\n",
    "- **Queries**: Help the model decide which part of the sequence to focus on for each specific token. By calculating the dot product between a query vector and all key vectors, the model assesses how much attention to give to each token relative to the current token.\n",
    "- **Keys**: Represent all the tokens in the sequence and are used to compare with the query vector to calculate the attention scores.\n",
    "- **Values**: Hold the actual information that will be aggregated to form the output of the attention mechanism.\n",
    "\n",
    "#### 3.3. Token Embeddings and Linear Transformation\n",
    "\n",
    "For example, consider the input sequence \"the cat sat.\" The embedding size is four, meaning each word is converted into four vectors. The first step is token embeddings, where the sentence is converted into vectors, V. The next step is linear transformation, where query, key, and value vectors are created by multiplying the embeddings by learned weight matrices.\n",
    "\n",
    "This process ensures that the model has information about all the other words and sentences, allowing it to create contextual embeddings based on the entire sentence. This is essential for capturing dependencies and context, which is crucial for applications like language translation and text summarization.\n",
    "\n",
    "#### 3.4. Practical Example\n",
    "\n",
    "Here they are in one line:\n",
    "\n",
    "Is there anything else you'd like to know about these weight matrices or their role in the attention mechanism?\n",
    "For a practical example, consider token embeddings. Let's initialize the weights $$ W_q, W_k, \\text{and} \\, W_v $$ as identity matrices. For instance, if the identity matrix is 3x3, it will have all diagonal elements as one and the rest as zeros. Using this identity matrix, the query, key, and value vectors for the word \"the\" will be the same as the original vectors after the dot operation.\n",
    "\n",
    "Similarly, for other words like \"cat\" and \"sat,\" the query, key, and value vectors are computed. This is the first step: obtaining token embeddings and creating Q, K, and V by multiplying the embeddings by learned weight matrices. Initially, these weights are not learned but will be adjusted through backpropagation.\n",
    "\n",
    "\n",
    "### 3.5. Attention Scores\n",
    "\n",
    "#### 3.5.1. **Computation of Attention Scores**:\n",
    "To calculate the dot product between a query vector and all key vectors, you follow these steps:\n",
    "\n",
    "1. **Identify the query and key vectors**: For each token, you have a corresponding query vector and key vector, $$ Q \\, \\text{and} \\, K $$\n",
    "\n",
    "2. **Compute the dot product**: For each pair of tokens, calculate the dot product of their query and key vectors.\n",
    "\n",
    "$$ \\text{Attention Score} = Q \\times K^T $$\n",
    "\n",
    "- **Attention score for \"the\" with respect to \"the\"**:\n",
    "  - Query vector for \"the\": $$ Q_{\\text{the}} $$\n",
    "  - Key vector for \"the\": $$ K_{\\text{the}} $$\n",
    "  - Dot product: $$ Q_{\\text{the}} \\cdot K_{\\text{the}}^T = 2 $$\n",
    "\n",
    "- **Attention score for \"the\" with \"cat\"**:\n",
    "  - Query vector for \"the\": $$ Q_{\\text{the}} $$\n",
    "  - Key vector for \"cat\": $$ K_{\\text{cat}} $$\n",
    "  - Dot product: $$ Q_{\\text{the}} \\cdot K_{\\text{cat}}^T $$\n",
    "\n",
    "- **Attention score for \"the\" with \"sat\"**:\n",
    "  - Query vector for \"the\": $$ Q_{\\text{the}} $$\n",
    "  - Key vector for \"sat\": $$ K_{\\text{sat}} $$\n",
    "  - Dot product: $$ Q_{\\text{the}} \\cdot K_{\\text{sat}}^T $$\n",
    "\n",
    "Each of these dot products gives you the attention score, indicating how much focus the model should place on each token relative to the current token \"the.\"\n",
    "\n",
    "\n",
    "\n",
    "#### 3.5.2. **Example for Token \"cat\"**:\n",
    "- Scores are computed similarly, indicating the importance of \"sat\" for \"cat.\"\n",
    "- The same process is repeated for \"sat,\" resulting in scores that reflect the dependencies between the tokens.\n",
    "\n",
    "### 3.6. Scaling\n",
    "\n",
    "#### 3.6.1. **Purpose of Scaling**:\n",
    "- Scaling is crucial to prevent the dot product from growing too large, ensuring stable gradients during training.\n",
    "- Scores are scaled down by dividing by the square root of the dimensions of the key vectors.\n",
    "- Example: If the dimension is 4, the square root is 2.\n",
    "\n",
    "    $$ \\text{Scaled Score} = \\frac{\\text{Attention Score}}{\\sqrt{d_k}} $$\n",
    "\n",
    "These scores help determine the importance of each token relative to the current token. The next step is to scale these scores to prevent issues like gradient exploding and softmax saturation. Scaling is done by dividing the scores by the square root of the dimensions of the key vectors. \n",
    "\n",
    "### 3.6.2. **Benefits of Scaling**:\n",
    "- **Prevents Gradient Exploding**: Scaling the attention scores helps prevent issues like gradient exploding during backpropagation, which can destabilize training.\n",
    "- **Maintains Stable Gradients**: By scaling the scores, the gradients remain stable, ensuring effective training.\n",
    "- **Avoids Softmax Saturation**: Without scaling, the dot products could become very large, pushing the softmax function into regions where it has extremely small gradients, leading to poor learning. Scaling helps keep the values in a range where the softmax function can operate effectively.\n",
    "\n",
    "### 3.7. Softmax and Attention Weights\n",
    "After scaling, the scores are passed through a softmax function to obtain the attention weights. The softmax function ensures that the attention weights sum to 1, providing a probability distribution over the tokens.\n",
    "\n",
    "The softmax formula is:\n",
    "$$ \\text{Attention Weights} = \\text{softmax}\\left(\\frac{Q \\cdot K^T}{\\sqrt{d_k}}\\right) $$\n",
    "\n",
    "### 3.8. Weighted Sum of Values\n",
    "The final step is to compute the weighted sum of the value vectors, using the attention weights as coefficients. This gives the final output of the self-attention mechanism. V is the embeding vectors.\n",
    "\n",
    "The weighted sum formula is:\n",
    "$$ \\text{Output} = \\sum (\\text{Attention Weights} \\cdot V) $$\n",
    "\n",
    "\n",
    "#### Recall of Value Vectors (V)\n",
    "The value vectors (V) are indeed related to the embedding vectors. In the context of the self-attention mechanism, the value vectors are derived from the input embeddings.\n",
    "- **Embedding Vectors**: These are the initial representations of the input tokens. Each token in the input sequence is mapped to a high-dimensional vector that captures its semantic meaning.\n",
    "- **Value Vectors (V)**: These are typically the same as the embedding vectors or a transformed version of them. In the self-attention mechanism, the value vectors contain the actual information that will be combined based on the attention weights.\n",
    "\n",
    "### Process\n",
    "1. **Input Embeddings**: The input tokens are first converted into embedding vectors.\n",
    "2. **Transformation (Optional)**: Sometimes, these embeddings are linearly transformed to produce the value vectors.\n",
    "3. **Attention Mechanism**: The attention mechanism computes the attention weights and uses them to combine the value vectors, resulting in the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.10. Self-Attention Mechanism: Detailed Mathematical Computation\n",
    "\n",
    "Let's dive into the detailed mathematical computation for the self-attention mechanism using the example sentence \"the cat sat.\"\n",
    "\n",
    "#### Step 1: Token Embeddings\n",
    "Assume the embedding vectors for the tokens are:\n",
    "- \"the\": $$[1, 0, 0]$$\n",
    "- \"cat\": $$[0, 1, 0]$$\n",
    "- \"sat\": $$[0, 0, 1]$$\n",
    "\n",
    "#### Step 2: Linear Transformation\n",
    "Assume the weight matrices $$W_q$$, $$W_k$$, and $$W_v$$ are identity matrices:\n",
    "- $$W_q = W_k = W_v = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "The query, key, and value vectors for each token are the same as the original vectors:\n",
    "- Query vector for \"the\": $$Q_{\\text{the}} = [1, 0, 0]$$\n",
    "- Key vector for \"the\": $$K_{\\text{the}} = [1, 0, 0]$$\n",
    "- Value vector for \"the\": $$V_{\\text{the}} = [1, 0, 0]$$\n",
    "\n",
    "Similarly, for \"cat\" and \"sat\":\n",
    "- Query vector for \"cat\": $$Q_{\\text{cat}} = [0, 1, 0]$$\n",
    "- Key vector for \"cat\": $$K_{\\text{cat}} = [0, 1, 0]$$\n",
    "- Value vector for \"cat\": $$V_{\\text{cat}} = [0, 1, 0]$$\n",
    "\n",
    "- Query vector for \"sat\": $$Q_{\\text{sat}} = [0, 0, 1]$$\n",
    "- Key vector for \"sat\": $$K_{\\text{sat}} = [0, 0, 1]$$\n",
    "- Value vector for \"sat\": $$V_{\\text{sat}} = [0, 0, 1]$$\n",
    "\n",
    "#### Step 3: Compute Attention Scores\n",
    "Compute the dot product between the query vector of each token and the key vectors of all tokens.\n",
    "\n",
    "For \"the\":\n",
    "- Attention score for \"the\" with respect to \"the\":\n",
    "  $$ Q_{\\text{the}} \\cdot K_{\\text{the}}^T = [1, 0, 0] \\cdot [1, 0, 0]^T = 1 \\times 1 + 0 \\times 0 + 0 \\times 0 = 1 $$\n",
    "\n",
    "- Attention score for \"the\" with respect to \"cat\":\n",
    "  $$ Q_{\\text{the}} \\cdot K_{\\text{cat}}^T = [1, 0, 0] \\cdot [0, 1, 0]^T = 1 \\times 0 + 0 \\times 1 + 0 \\times 0 = 0 $$\n",
    "\n",
    "- Attention score for \"the\" with respect to \"sat\":\n",
    "  $$ Q_{\\text{the}} \\cdot K_{\\text{sat}}^T = [1, 0, 0] \\cdot [0, 0, 1]^T = 1 \\times 0 + 0 \\times 0 + 0 \\times 1 = 0 $$\n",
    "\n",
    "For \"cat\":\n",
    "- Attention score for \"cat\" with respect to \"the\":\n",
    "  $$ Q_{\\text{cat}} \\cdot K_{\\text{the}}^T = [0, 1, 0] \\cdot [1, 0, 0]^T = 0 \\times 1 + 1 \\times 0 + 0 \\times 0 = 0 $$\n",
    "\n",
    "- Attention score for \"cat\" with respect to \"cat\":\n",
    "  $$ Q_{\\text{cat}} \\cdot K_{\\text{cat}}^T = [0, 1, 0] \\cdot [0, 1, 0]^T = 0 \\times 0 + 1 \\times 1 + 0 \\times 0 = 1 $$\n",
    "\n",
    "- Attention score for \"cat\" with respect to \"sat\":\n",
    "  $$ Q_{\\text{cat}} \\cdot K_{\\text{sat}}^T = [0, 1, 0] \\cdot [0, 0, 1]^T = 0 \\times 0 + 1 \\times 0 + 0 \\times 1 = 0 $$\n",
    "\n",
    "For \"sat\":\n",
    "- Attention score for \"sat\" with respect to \"the\":\n",
    "  $$ Q_{\\text{sat}} \\cdot K_{\\text{the}}^T = [0, 0, 1] \\cdot [1, 0, 0]^T = 0 \\times 1 + 0 \\times 0 + 1 \\times 0 = 0 $$\n",
    "\n",
    "- Attention score for \"sat\" with respect to \"cat\":\n",
    "  $$ Q_{\\text{sat}} \\cdot K_{\\text{cat}}^T = [0, 0, 1] \\cdot [0, 1, 0]^T = 0 \\times 0 + 0 \\times 1 + 1 \\times 0 = 0 $$\n",
    "\n",
    "- Attention score for \"sat\" with respect to \"sat\":\n",
    "  $$ Q_{\\text{sat}} \\cdot K_{\\text{sat}}^T = [0, 0, 1] \\cdot [0, 0, 1]^T = 0 \\times 0 + 0 \\times 0 + 1 \\times 1 = 1 $$\n",
    "\n",
    "#### Step 4: Scaling\n",
    "Scale the attention scores by dividing by the square root of the dimension of the key vectors ($$d_k$$). Assuming $$d_k = 3$$, the square root of 3 is approximately 1.732.\n",
    "\n",
    "For \"the\":\n",
    "- Scaled score for \"the\" with respect to \"the\":\n",
    "  $$ \\frac{1}{\\sqrt{3}} \\approx 0.577 $$\n",
    "\n",
    "- Scaled score for \"the\" with respect to \"cat\":\n",
    "  $$ \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "- Scaled score for \"the\" with respect to \"sat\":\n",
    "  $$ \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "For \"cat\":\n",
    "- Scaled score for \"cat\" with respect to \"the\":\n",
    "  $$ \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "- Scaled score for \"cat\" with respect to \"cat\":\n",
    "  $$ \\frac{1}{\\sqrt{3}} \\approx 0.577 $$\n",
    "\n",
    "- Scaled score for \"cat\" with respect to \"sat\":\n",
    "  $$ \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "For \"sat\":\n",
    "- Scaled score for \"sat\" with respect to \"the\":\n",
    "  $$ \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "- Scaled score for \"sat\" with respect to \"cat\":\n",
    "  $$ \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "- Scaled score for \"sat\" with respect to \"sat\":\n",
    "  $$ \\frac{1}{\\sqrt{3}} \\approx 0.577 $$\n",
    "\n",
    "#### Step 5: Softmax and Attention Weights\n",
    "Apply the softmax function to the scaled scores to obtain the attention weights.\n",
    "\n",
    "For \"the\":\n",
    "- Attention weights:\n",
    "  $$ \\text{softmax}([0.577, 0, 0]) = \\left[\\frac{e^{0.577}}{e^{0.577} + e^0 + e^0}, \\frac{e^0}{e^{0.577} + e^0 + e^0}, \\frac{e^0}{e^{0.577} + e^0 + e^0}\\right] $$\n",
    "  $$ \\approx [0.491, 0.255, 0.255] $$\n",
    "\n",
    "For \"cat\":\n",
    "- Attention weights:\n",
    "  $$ \\text{softmax}([0, 0.577, 0]) = \\left[\\frac{e^0}{e^0 + e^{0.577} + e^0}, \\frac{e^{0.577}}{e^0 + e^{0.577} + e^0}, \\frac{e^0}{e^0 + e^{0.577} + e^0}\\right] $$\n",
    "  $$ \\approx [0.255, 0.491, 0.255] $$\n",
    "\n",
    "For \"sat\":\n",
    "- Attention weights:\n",
    "  $$ \\text{softmax}([0, 0, 0.577]) = \\left[\\frac{e^0}{e^0 + e^0 + e^{0.577}}, \\frac{e^0}{e^0 + e^0 + e^{0.577}}, \\frac{e^{0.577}}{e^0 + e^0 + e^{0.577}}\\right] $$\n",
    "  $$ \\approx [0.255, 0.255, 0.491] $$\n",
    "\n",
    "#### Step 6: Weighted Sum of Values\n",
    "Compute the weighted sum of the value vectors using the attention weights.\n",
    "\n",
    "For \"the\":\n",
    "- Output vector:\n",
    "  $$ \\text{Output}_{\\text{the}} = 0.491 \\times [1, 0, 0] + 0.255 \\times [0, 1, 0] + 0.255 \\times [0, 0, 1] $$\n",
    "  $$\n",
    "\n",
    "\n",
    "#### 3.11. Summary\n",
    "Without scaling, the dot product of query and key vectors can result in very high values, leading to issues like gradient exploding and softmax saturation. For example, if the scores are 6 and 4, applying softmax without scaling results in a large difference in attention weights, such as 88 and 0.12. This imbalance can cause vanishing gradient problems during backpropagation.\n",
    "\n",
    "By scaling the scores, the attention weights become more balanced. For example, scaling the scores 6 and 4 by dividing by the square root of 4 results in 3 and 2. Applying softmax to these scaled scores results in more balanced attention weights, such as 0.73 and 0.27. This balance prevents vanishing gradient problems and ensures stable training.\n",
    "\n",
    "In summary, scaling stabilizes the training process by preventing extremely large dot products and ensuring balanced attention weights. This is crucial for the effective operation of the self-attention mechanism in the Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Multi-Head Attention in Transformers\n",
    "\n",
    "### 4.1 Multi-Head Attention in Transformers\n",
    "\n",
    "Multi-head attention allows the model to capture more information by focusing on different parts of the input. This section covers the entire process of multi-head attention. In the next section, the process of passing these combined vectors to the feed-forward neural network and performing forward and backward propagation will be discussed.\n",
    "\n",
    "### 4.2 Steps in Self-Attention\n",
    "\n",
    "First, the query, key, and value vectors are calculated from learned weights \\(W_q\\), \\(W_k\\), and \\(W_v\\):\n",
    "\n",
    "$$\n",
    "Q = XW_q, \\quad K = XW_k, \\quad V = XW_v\n",
    "$$\n",
    "\n",
    "Then, the attention score is calculated by taking the dot product of the query and key vectors:\n",
    "\n",
    "$$\n",
    "\\text{Attention Score} = QK^T\n",
    "$$\n",
    "\n",
    "The scores are scaled by the square root of the dimension of the key vectors (\\(d_k\\)):\n",
    "\n",
    "$$\n",
    "\\text{Scaled Scores} = \\frac{QK^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "A softmax activation function is applied to obtain the attention weights:\n",
    "\n",
    "$$\n",
    "\\text{Attention Weights} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)\n",
    "$$\n",
    "\n",
    "Finally, the weighted sum of the values is calculated by multiplying the attention weights with the value vectors:\n",
    "\n",
    "$$\n",
    "Z = \\text{Attention Weights} \\cdot V\n",
    "$$\n",
    "\n",
    "### 4.3 Contextual Vectors\n",
    "\n",
    "Initially, when embedding vectors are given, contextual vectors based on the context and the dependency of the other words in the sentence should be obtained.\n",
    "\n",
    "### 4.4 Multi-Head Attention\n",
    "\n",
    "Initially, words such as \"the\" and \"cat\" are used. All vectors will be available here. Multiplication and a dot operation with \\(W_k\\) are performed to get the query vectors, key vectors, and value vectors. For each word, separate query, key, and value vectors are obtained. After getting these, a multiplication operation with the query and key is performed, divided by \\(\\sqrt{d_k}\\) for scaling, a softmax activation function is applied, and finally, a dot operation with the value vector is performed to get the \\(Z\\) value, which is the self-attention for the word.\n",
    "\n",
    "### 4.5 Purpose of Multi-Head Attention\n",
    "\n",
    "The idea with multi-head attention is to have self-attention with multiple heads. For the same words, multiple attention heads can be created. For example, with \"thinking machine,\" weights like \n",
    "\n",
    "$$\n",
    "XW_q, XW_k, XW_v\n",
    "$$\n",
    "\n",
    "are initialized, and \n",
    "\n",
    "$$\n",
    "q_1, k_1, v_1\n",
    "$$\n",
    "\n",
    "are calculated. Each set of vectors may capture different contextual information. This expands the model's ability to focus on different positions of words or tokens.\n",
    "\n",
    "### 4.6 Multiple Sets of Key-Value Pairs\n",
    "\n",
    "In multi-head attention, multiple sets of key-value pairs are used, each randomly initialized initially. After training, each set projects the input embeddings into different representational subspaces. By using this, multiple attention heads are obtained, each capturing different aspects of the input.\n",
    "\n",
    "### 4.7 Combining Attention Heads\n",
    "\n",
    "After performing self-attention, this information needs to be provided to the feed-forward neural network. Instead of getting one attention head, multiple heads are obtained. These need to be combined before passing them to the feed-forward neural network. All the attention heads are concatenated and a dot product with \\(W_o\\) is performed to get the final \\(Z\\):\n",
    "\n",
    "$$\n",
    "Z = \\text{Concat}(Z_1, Z_2, \\ldots, Z_h)W_o\n",
    "$$\n",
    "\n",
    "### 4.8 Self-Attention Mechanism\n",
    "\n",
    "In the self-attention mechanism, each word in the input sequence is transformed into three vectors: Query (Q), Key (K), and Value (V). These vectors are obtained by multiplying the input embeddings by learned weight matrices:\n",
    "\n",
    "$$\n",
    "Q = XW^Q, \\quad K = XW^K, \\quad V = XW^V\n",
    "$$\n",
    "\n",
    "where \\( X \\) is the input embedding, and \\( W^Q \\), \\( W^K \\), and \\( W^V \\) are the weight matrices for the query, key, and value vectors, respectively.\n",
    "\n",
    "The attention scores are calculated by taking the dot product of the query vector with all key vectors, followed by a softmax function to obtain the attention weights:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "where \\( d_k \\) is the dimension of the key vectors, and the softmax function ensures that the attention weights sum to 1.\n",
    "\n",
    "### 4.9 Multi-Head Attention\n",
    "\n",
    "Multi-head attention allows the model to focus on different parts of the input sequence simultaneously. For each word, multiple attention heads are created, each with its own set of weight matrices. The outputs of these attention heads are concatenated and linearly transformed:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "W^O\n",
    "$$\n",
    "\n",
    "is the output weight matrix.\n",
    "\n",
    "### 4.10 Combining Attention Heads\n",
    "\n",
    "The next step involves passing these vectors to a feed-forward neural network for further processing within the encoder. Initially, a single word was passed to get one vector, but now multiple vectors are obtained. Before passing through the feed-forward neural network, these vectors need to be combined into a single matrix along with some weights, and then the forward and backward propagation can be performed.\n",
    "\n",
    "The feed-forward neural network expects a single matrix, so a method is needed to condense all these attention heads into a single matrix with specific weights. This is achieved by concatenating all the attention heads and performing a dot product with \\( W_0 \\), initializing a new weight for the feed-forward neural network:\n",
    "\n",
    "$$\n",
    "Z = \\text{Concat}(Z_0, Z_1, Z_2, \\ldots, Z_7)W_0\n",
    "$$\n",
    "\n",
    "### 4.11 Positional Encoding\n",
    "\n",
    "One major advantage of using Transformers is the ability to process word tokens in parallel. However, this advantage also introduces a drawback: the lack of sequential structure. The order of words is crucial, as it changes the meaning of sentences. For example, \"lion kills tiger\" and \"tiger kills lion\" have different meanings due to word order.\n",
    "\n",
    "To address this, positional encoding is used to represent the order of sequences. According to the research paper \"Attention is All You Need,\" a positional encoded vector is created and added to the embedding vector of each word. This positional encoded vector indicates the position of each word in the sequence.\n",
    "\n",
    "The positional encoding for each position \\( pos \\) and dimension \\( i \\) is defined as:\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "where \\( d_{model} \\) is the dimension of the model. These sinusoidal functions ensure that each position has a unique encoding, and similar positions have similar encodings.\n",
    "\n",
    "The positional encoded vector is then added to the input embedding vector:\n",
    "\n",
    "$$\n",
    "X' = X + PE\n",
    "$$\n",
    "\n",
    "This combined vector \\( X' \\) is then used in the self-attention mechanism, allowing the model to incorporate the positional information of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "[1] Complete Transformers For NLP Deep Learning One Shot With Handwritten Notes by Krish Naik, https://www.youtube.com/watch?v=3bPhDUSAUYI\n",
    "\n",
    "[2] https://arxiv.org/pdf/1706.03762"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
