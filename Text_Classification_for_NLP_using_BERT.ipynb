{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification for NLP using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. The Role of Transformers in Revolutionizing Natural Language Processing**\n",
    "Transformers are a type of neural network architecture that has revolutionized the field of Natural Language Processing (NLP). Introduced by researchers from Google in 2017 through the paper \"Attention Is All You Need,\" transformers have become the foundation for many advanced language models, such as BERT and GPT-3.\n",
    "\n",
    "Transformers excel in various NLP tasks due to their ability to handle long-range dependencies in text and their use of self-attention mechanisms. Here are some key applications of transformers in NLP:\n",
    "\n",
    "1. **Sentence Classification**: Transformers can classify the sentiment of entire sentences. For example, given a review of an airline, the model can determine whether the sentiment is positive or negative based on the text.\n",
    "\n",
    "2. **Named Entity Recognition (NER)**: This task involves identifying and classifying entities within a sentence, such as names of people, organizations, or locations. Transformers can accurately recognize entities even when they consist of multiple words, like \"Singapore Airlines\" or \"Chew Choon Seng.\"\n",
    "\n",
    "3. **Question Answering**: Transformers can extract answers from a given context. For instance, when provided with a text about Singapore Airlines, the model can answer questions about the number of aircraft in the fleet by locating the relevant information within the text.\n",
    "\n",
    "4. **Text Summarization**: This involves condensing a long piece of text into a shorter summary while retaining the main points. Transformers can generate concise summaries that capture the essence of the original text.\n",
    "\n",
    "5. **Fill-in-the-Blank Tasks**: Transformers can predict missing words in a sentence. For example, given the sentence \"Singapore Airlines is the national <mask> of Singapore,\" the model can correctly predict \"airline\" as the missing word.\n",
    "\n",
    "6. **Language Translation**: Transformers are highly effective in translating text between different languages. They can handle complex linguistic structures and provide accurate translations.\n",
    "\n",
    "The success of transformers in these tasks is attributed to their architecture, which allows them to focus on different parts of the input text through self-attention. This enables them to understand context and relationships between words more effectively than previous models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Transformers in Production**\n",
    "\n",
    "Transformers have significantly enhanced the capabilities of various applications, particularly in search engines. Here are some key examples of how transformers, specifically BERT (Bidirectional Encoder Representations from Transformers), are used in production:\n",
    "\n",
    "1. **Improved Search Query Understanding**:\n",
    "   - **Example**: Searching for \"curling objective\" vs. \"what's the main objective for curling in the Olympics?\"\n",
    "   - **Impact**: BERT allows the search engine to understand the context and nuances of more complex, natural language queries, providing more accurate and relevant answers directly in the search results.\n",
    "\n",
    "2. **Enhanced Contextual Relevance**:\n",
    "   - **Example**: Searching for \"can you get medicine for someone pharmacy.\"\n",
    "   - **Impact**: BERT captures the important nuance of \"for someone,\" returning results about having another person pick up the medicine, rather than general prescription information.\n",
    "\n",
    "3. **Direct Answer Extraction**:\n",
    "   - **Example**: Queries like \"what's the main objective for curling in the Olympics?\" result in the answer being highlighted in bold within the search results.\n",
    "   - **Impact**: This is a direct application of the question-answering capability of transformers, where the answer is extracted from the context and presented prominently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. History of Transformers in NLP**\n",
    "\n",
    "The evolution of transformer models in Natural Language Processing (NLP) has been remarkable since their introduction in 2017. Here is a chronological overview of key developments:\n",
    "\n",
    "1. **2017: Introduction of Transformers**\n",
    "   - **Paper**: \"Attention Is All You Need\" by Google researchers.\n",
    "   - **Significance**: Introduced the transformer architecture, which revolutionized NLP by enabling models to handle long-range dependencies and context more effectively.\n",
    "\n",
    "2. **2018: ULMFiT and GPT**\n",
    "   - **ULMFiT**: Proposed by Jeremy Howard and Sebastian Ruder, this model allowed training without labeled data, utilizing large text corpora like Wikipedia.\n",
    "   - **GPT (Generative Pre-training Transformer)**: Developed by OpenAI, this was the first pre-trained transformer model, achieving state-of-the-art results in various NLP tasks through fine-tuning.\n",
    "\n",
    "3. **2018: BERT**\n",
    "   - **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT enabled better understanding of context in search queries and other NLP applications.\n",
    "\n",
    "4. **2019: GPT-2 and Other Models**\n",
    "   - **GPT-2**: Released by OpenAI, this model was notable for its size and capabilities, though its release was initially restricted due to ethical concerns.\n",
    "   - **BART and T5**: Released by Facebook AI Research and Google, respectively, these models continued the trend of large pre-trained transformers.\n",
    "\n",
    "5. **2019: DistilBERT**\n",
    "   - **DistilBERT**: Released by Hugging Face, this model was a smaller, faster, and lighter version of BERT, retaining 95% of BERT's performance while reducing its size by 40%.\n",
    "\n",
    "6. **2020: GPT-3**\n",
    "   - **GPT-3**: OpenAI's third revision of the GPT model, known for generating high-quality English sentences. Despite detailed documentation, the dataset and weights were not released.\n",
    "\n",
    "7. **2021-2022: EleutherAI Models**\n",
    "   - **GPT-Neo**: Released in March 2021 with 2 billion parameters.\n",
    "   - **GPT-J**: Released a few months later with 6 billion parameters.\n",
    "   - **GPT-NeoX**: Released in February 2022 with 20 billion parameters.\n",
    "\n",
    "8. **Parameter Growth**\n",
    "   - **Trend**: The number of parameters in transformer models has increased exponentially. For example, BERT has around 110 million parameters, BERT Large has 340 million, GPT-2 has 1.5 billion, and GPT-3 has 175 billion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Understanding BERT Model Sizes**\n",
    "   - A checkpoint includes the model configuration and pre-trained weights.\n",
    "   - Common checkpoints for BERT are:\n",
    "     - **BERT Base Cased**: Distinguishes between upper and lowercase words.\n",
    "     - **BERT Base Uncased**: Does not distinguish between upper and lowercase words.\n",
    "\n",
    "1. **Memory Requirement for BERT Base Cased Inference**:\n",
    "   - The BERT base cased model has approximately **108 million parameters**.\n",
    "   - Each parameter is represented as a 4-byte floating point.\n",
    "   - To calculate the memory required:\n",
    "     - Formula: `Memory (in bytes) = Number of Parameters * 4`\n",
    "     - For BERT base cased: `108 million parameters * 4 bytes = 432 megabytes`\n",
    "   - Therefore, running an inference with the BERT base cased model requires approximately **432 megabytes of RAM**.\n",
    "\n",
    "2. **RAM Requirement for GPT-3 Inference**:\n",
    "   - GPT-3 has 175 billion parameters.\n",
    "   - Using the same formula:\n",
    "     - `175 billion parameters * 4 bytes = 700 gigabytes`\n",
    "   - Therefore, running an inference with the GPT-3 model requires approximately **700 gigabytes of RAM**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Bias in BERT**\n",
    "\n",
    "Exploring the potential biases in BERT (Bidirectional Encoder Representations from Transformers) is crucial before deploying it in production. Here are some key points to consider:\n",
    "\n",
    "1. **Gender Bias in Predictions**:\n",
    "   - **Example 1**: \"The nurse needed a drink because [mask] was tired after a long day's work at the hospital.\"\n",
    "     - BERT predicts a higher probability that the nurse is female (96%) compared to male (2%).\n",
    "   - **Example 2**: \"The doctor needed a drink because [mask] was tired after a long day's work at the hospital.\"\n",
    "     - BERT predicts a higher probability that the doctor is male (93%) compared to female (5%).\n",
    "\n",
    "2. **Occupational Stereotypes**:\n",
    "   - **Example 3**: \"We had a meeting with our company receptionist and [mask] was not happy.\"\n",
    "     - BERT predicts a higher probability that the receptionist is female (88%) compared to male (2%).\n",
    "   - **Example 4**: \"We had a meeting with our company president and [mask] was not happy.\"\n",
    "     - BERT predicts a higher probability that the president is male (92%) compared to female (6%).\n",
    "   - **Example 5**: \"The programmer stepped away from the computer because [mask] wanted a break.\"\n",
    "     - BERT predicts a higher probability that the programmer is male (96%) compared to female (3%).\n",
    "\n",
    "3. **Implications of Bias**:\n",
    "   - Lower-skilled and lower-paid jobs are more readily linked to women, while higher-skilled and higher-paid jobs are more readily linked to men.\n",
    "   - This bias can affect downstream tasks, such as resume filtering by AI systems, potentially leading to gender discrimination in hiring processes.\n",
    "\n",
    "4. **Need for Human Oversight**:\n",
    "   - It is essential to have human oversight to check the output of BERT-based models, especially for tasks that can have significant social implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. How BERT Was Trained**\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) was trained on two large datasets:\n",
    "\n",
    "1. **English Wikipedia**: Approximately 2.5 billion words.\n",
    "2. **BookCorpus**: A collection of 11,000 books by unpublished authors, containing around 800 million words.\n",
    "\n",
    "The training process involved two key tasks:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**:\n",
    "   - In this task, some words in a sentence are masked, and BERT is trained to predict these masked words.\n",
    "   - Example: \"BERT is conceptually [mask] and empirically powerful.\" BERT needs to predict the masked word \"simple.\"\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**:\n",
    "   - This task involves determining if a given sentence logically follows another sentence.\n",
    "   - Example: Given the sentences \"BERT is conceptually simple and empirically powerful.\" and \"It obtains new state-of-the-art results,\" BERT needs to decide if the second sentence follows the first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **7. Transfer Learning**\n",
    "\n",
    "Transfer learning consists of two main components: pre-training and fine-tuning.\n",
    "\n",
    "1. **Pre-training**:\n",
    "   - **Starting Point**: The model architecture with random weights, meaning the model initially has no knowledge of language.\n",
    "   - **Process**: The model is pre-trained on large datasets, such as the entire Wikipedia corpus and other extensive corpora. This process is resource-intensive, requiring significant computational power, typically involving hundreds to thousands of hardware accelerators like Nvidia GPUs or Google TPUs.\n",
    "   - **Outcome**: After days, weeks, or months of training, the model gains a robust understanding of the language it was trained on.\n",
    "\n",
    "2. **Fine-tuning**:\n",
    "   - **Using Pre-trained Models**: Instead of starting from scratch, the pre-trained model (e.g., BERT) with a good understanding of language is used as the starting point.\n",
    "   - **Task-Specific Training**: The model is fine-tuned for specific tasks such as text classification, named entity recognition, or question answering using labeled data.\n",
    "   - **Example**: For sentiment analysis, the model is trained with text examples labeled as positive or negative.\n",
    "\n",
    "**Benefits of Transfer Learning**:\n",
    "- **Efficiency**: Fine-tuning requires significantly less time compared to pre-training. For BERT, fine-tuning typically involves 2 to 4 epochs of training.\n",
    "- **Data Requirements**: Fine-tuning does not require another massive dataset, unlike pre-training which uses large corpora like Wikipedia.\n",
    "- **Performance**: Models pre-trained on large datasets and then fine-tuned for specific tasks generally achieve better accuracy than models trained from scratch.\n",
    "\n",
    "**Pre-training Tasks for BERT**:\n",
    "- **Masked Language Modeling (MLM)**: BERT predicts masked words in sentences.\n",
    "  - Example: \"BERT is conceptually [mask] and empirically powerful.\"\n",
    "- **Next Sentence Prediction (NSP)**: BERT predicts whether one sentence follows another.\n",
    "  - Example: \"BERT is conceptually simple and empirically powerful.\" followed by \"It obtains new state-of-the-art results.\"\n",
    "\n",
    "**Comparison of Pre-training for Larger Models**:\n",
    "- **BERT (2018)**:\n",
    "  - Parameters: 109 million\n",
    "  - Training Time: 12 days on TPUs\n",
    "  - Dataset Size: 16 GB\n",
    "  - Training Tokens: 250 billion\n",
    "  - Data Sources: Wikipedia, BookCorpus\n",
    "\n",
    "- **RoBERTa (2019)**:\n",
    "  - Parameters: 125 million\n",
    "  - Training Time: 1 day on 1,024 V100 GPUs\n",
    "  - Dataset Size: 160 GB\n",
    "  - Training Tokens: 2,000 billion\n",
    "  - Data Sources: Wikipedia, BookCorpus, Common Crawl news, OpenWebText, Common Crawl stories\n",
    "\n",
    "- **GPT-3 (2020)**:\n",
    "  - Parameters: 165 billion\n",
    "  - Training Time: ~34 days on 10,000 V100 GPUs\n",
    "  - Dataset Size: 4,500 GB\n",
    "  - Training Tokens: 300 billion\n",
    "  - Data Sources: Wikipedia, Common Crawl, WebText2, Books1, Books2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **8. Transformer Architecture Overview**\n",
    "\n",
    "The transformer architecture, introduced in the \"Attention Is All You Need\" paper, consists of two main components: the encoder and the decoder.\n",
    "\n",
    "1. **Components**:\n",
    "   - **Encoder**: Processes the input sentence. For example, \"I like NLP\" is fed into the encoder.\n",
    "   - **Decoder**: Generates the output sentence. For instance, the German translation \"ich mag NLP\" is produced by the decoder.\n",
    "\n",
    "2. **Structure**:\n",
    "   - The transformer is composed of multiple layers of encoders and decoders. Typically, there are six encoders and six decoders.\n",
    "   - Each encoder and decoder can be used independently depending on the task.\n",
    "\n",
    "3. **Types of Models**:\n",
    "   - **Encoder-Decoder Models**: Suitable for generative tasks that require an input, such as translation or summarization. Examples include Facebook's BART and Google's T5.\n",
    "   - **Encoder-Only Models**: Ideal for tasks that require understanding of the input, such as sentence classification and named entity recognition. Examples include BERT, RoBERTa, and DistilBERT.\n",
    "   - **Decoder-Only Models**: Used for generative tasks like text generation. Examples include the GPT family (GPT, GPT-2, GPT-3).\n",
    "\n",
    "4. **BERT's Capabilities and Limitations**:\n",
    "   - **Capabilities**: BERT excels in tasks requiring input understanding, such as text classification, named entity recognition, and question answering.\n",
    "   - **Limitations**: BERT cannot generate text as it lacks the decoder component, making it unsuitable for tasks like text translation and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. BERT Model and Tokenization**\n",
    "\n",
    "When using BERT, it may seem like English sentences are processed directly, but under the hood, each word is split into subwords and mapped to numerical IDs. This process is essential because models can only process numerical data. Tokenizers convert text inputs into numerical data.\n",
    "\n",
    "1. **Subword Tokenization**:\n",
    "   - **Example**: The word \"tokenization\" is split into \"token\" and \"##ization\". The double hash indicates that \"ization\" should be merged with the previous token when converting back to a string.\n",
    "   - **Process**: The word piece tokenizer uses a greedy longest-match-first approach. It searches for the longest subword in the vocabulary, progressively shortening the word until a match is found.\n",
    "\n",
    "2. **Greedy Longest-Match-First**:\n",
    "   - The tokenizer first looks for the entire word \"tokenization\" in the vocabulary.\n",
    "   - If not found, it removes the last character and searches for \"tokenizatio\", and continues this process.\n",
    "   - Once \"token\" is found, it adds \"##ization\" and checks if it is in the vocabulary.\n",
    "\n",
    "3. **Token IDs**:\n",
    "   - Each subword in the vocabulary has a corresponding token ID, a numerical representation used by the model.\n",
    "\n",
    "4. **Objectives of Subword Tokenization**:\n",
    "   - **Frequent Words**: Commonly used words are not split into smaller subwords.\n",
    "   - **Rare Words**: Less common words are decomposed into meaningful subwords.\n",
    "\n",
    "5. **Vocabulary Sizes and Techniques**:\n",
    "   - **BERT Uncased**: Approximately 30,000 tokens, using word piece tokenization.\n",
    "   - **GPT-2 and GPT-3**: Around 50,000 tokens, using byte-pair encoding (BPE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **9. Positional Encodings and Segment Embeddings in BERT**\n",
    "\n",
    "When inputting a sentence into BERT, several steps are involved to prepare the text for processing:\n",
    "\n",
    "1. **Tokenization**:\n",
    "   - **Lowercasing**: If using a BERT base uncased checkpoint, all words are converted to lowercase.\n",
    "   - **Subword Tokens**: Words are split into subwords. For example, \"NLP\" is split into \"nl\" and \"##p\".\n",
    "   - **Special Tokens**: The tokenizer adds special tokens:\n",
    "     - **[CLS]**: Added at the beginning for sentence-level classification.\n",
    "     - **[SEP]**: Added to separate sentences in tasks involving multiple sentences.\n",
    "\n",
    "2. **Token IDs**:\n",
    "   - Each subword is mapped to a numerical ID, as models can only process numbers.\n",
    "\n",
    "3. **BERT Architecture**:\n",
    "   - BERT consists of 12 layers of encoders (the original transformer model had six encoders).\n",
    "   - The output of these encoders are the hidden states.\n",
    "\n",
    "4. **Bidirectional Context**:\n",
    "   - BERT processes the entire input sentence at once, allowing each word to see all other words in the sentence. This is in contrast to decoder-only models like GPT, which generate words sequentially and only have access to previously generated words.\n",
    "\n",
    "5. **Input Sequence Handling**:\n",
    "   - Input sequences are padded or truncated to a fixed length (BERT supports up to 512 tokens).\n",
    "\n",
    "6. **Segment Embeddings**:\n",
    "   - Used to differentiate between two pieces of text. The [SEP] token separates the texts, and segment embeddings (or token type IDs) are added to distinguish between the first and second sentences.\n",
    "\n",
    "7. **Positional Encodings**:\n",
    "   - Added to the embedding vector to provide a notion of word order.\n",
    "   - Ensure that tokens are closer to each other based on both the similarity of their meaning and their position in the sentence.\n",
    "\n",
    "**Example Workflow**:\n",
    "- Input text: \"I like NLP\"\n",
    "- Lowercased and tokenized: \"i\", \"like\", \"nl\", \"##p\"\n",
    "- Special tokens added: \"[CLS] i like nl ##p [SEP]\"\n",
    "- Converted to token IDs and segment embeddings.\n",
    "- Positional encodings added to capture word meanings and positions.\n",
    "- Processed through the 12 encoders, resulting in hidden states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **10. Tokenizers in BERT**\n",
    "\n",
    "Tokenizers are essential for converting text into a format that can be processed by the BERT model. Here's a detailed look at how tokenization works:\n",
    "\n",
    "1. **Setup**:\n",
    "   - Install the transformers library.\n",
    "   - Use the `bert-base-uncased` checkpoint, which has a vocabulary size of 30,522 tokens.\n",
    "\n",
    "2. **Tokenization Process**:\n",
    "   - **Example Sentence**: \"I like NLP.\"\n",
    "   - **WordPiece Tokenization**: Converts all tokens to lowercase and splits words into subwords. For instance, \"NLP\" becomes \"nl\" and \"##p\".\n",
    "   - **Special Tokens**: Adds `[CLS]` at the beginning and `[SEP]` at the end of the sentence.\n",
    "     - `[CLS]` token ID: 101\n",
    "     - `[SEP]` token ID: 102\n",
    "\n",
    "3. **Handling Unknown Tokens**:\n",
    "   - If a Unicode character (e.g., a grinning-face emoji) is not in the vocabulary, the tokenizer maps the entire word to an unknown token (`[UNK]`).\n",
    "\n",
    "4. **Tokenizing Multiple Sentences**:\n",
    "   - Use the `return_tensors='pt'` flag to return PyTorch tensors.\n",
    "   - The tokenizer returns a dictionary with three keys:\n",
    "     - **input_ids**: Numerical IDs of the tokens.\n",
    "     - **token_type_ids**: Segment IDs to distinguish between sentences (0s for the first sentence, 1s for the second).\n",
    "     - **attention_mask**: Indicates which tokens should be attended to (1s for actual tokens, 0s for padding).\n",
    "\n",
    "5. **Padding and Attention Mask**:\n",
    "   - When tokenizing a batch of sequences, shorter sequences are padded to match the length of the longest sequence.\n",
    "   - The attention mask ensures that padding tokens are ignored during processing.\n",
    "\n",
    "**Example Code**:\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Setup\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenization example\n",
    "sentence = 'I like NLP'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "ids = tokenizer.encode(sentence)\n",
    "decoded_sentence = tokenizer.decode(ids)\n",
    "\n",
    "print(f\"Sentence: {sentence}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {ids}\")\n",
    "print(f\"Decoded Sentence: {decoded_sentence}\")\n",
    "\n",
    "# Handling unknown tokens\n",
    "emoji_sentence = 'I like NLP😀'\n",
    "emoji_tokens = tokenizer.tokenize(emoji_sentence)\n",
    "print(f\"Tokens with emoji: {emoji_tokens}\")\n",
    "\n",
    "# Tokenizing multiple sentences\n",
    "first_sentence = 'I like NLP.'\n",
    "second_sentence = 'What about you?'\n",
    "input = tokenizer(first_sentence, second_sentence, return_tensors='pt')\n",
    "print(input)\n",
    "\n",
    "# Padding example\n",
    "first_sentence = 'I like NLP.'\n",
    "second_sentence = 'What are your thoughts on the subject?'\n",
    "input = tokenizer([first_sentence, second_sentence], padding=True, return_tensors='pt')\n",
    "print(input['attention_mask'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Self-Attention in Transformers**\n",
    "\n",
    "Self-attention is a fundamental mechanism in transformers that allows models to understand the relationships between words in a sentence. Here's a detailed explanation, including formulas and additional information:\n",
    "\n",
    "1. **Understanding Context**:\n",
    "   - **Example Sentence**: \"The monkey ate the banana because it was too hungry.\"\n",
    "   - **Challenge**: Determining that \"it\" refers to \"the monkey\" and not \"the banana.\"\n",
    "\n",
    "2. **Mechanism**:\n",
    "   - **Self-Attention**: Incorporates embeddings of all other words in the sentence to determine the context.\n",
    "   - When processing the word \"it,\" self-attention takes a weighted average of the embeddings of other context words.\n",
    "   - Words like \"monkey\" and \"banana\" are given different weights, with \"monkey\" receiving a higher weight if it is more relevant.\n",
    "\n",
    "3. **Under the Hood**:\n",
    "   - **Query, Key, and Value Vectors**: Word embeddings are projected into three vector spaces: query ($Q$), key ($K$), and value ($V$).\n",
    "   - **Calculating Attention Weights**:\n",
    "     - The dot product of the query and key vectors is calculated to determine the focus on other words.\n",
    "     - Similar queries and keys have a larger dot product, indicating more focus.\n",
    "     - The result is scaled by dividing by the square root of the dimension of the vectors ($d_k$).\n",
    "     - Softmax function converts these scores into probabilities.\n",
    "\n",
    "4. **Formulas**:\n",
    "   - **Dot Product Attention**:\n",
    "     $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n",
    "     - $Q$: Query matrix\n",
    "     - $K$: Key matrix\n",
    "     - $V$: Value matrix\n",
    "     - $d_k$: Dimension of the key vectors\n",
    "   - **Softmax Function**:\n",
    "     $$ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $$\n",
    "     - Converts logits into probabilities.\n",
    "\n",
    "5. **Weighted Sum**:\n",
    "   - Each value vector is multiplied by its corresponding softmax score.\n",
    "   - The weighted value vectors are summed to produce the self-attention calculation for each word.\n",
    "\n",
    "6. **Parallel Processing**:\n",
    "   - This process occurs for every word in the sentence, allowing the model to apply different weights to words based on their relevance.\n",
    "\n",
    "**Example Workflow**:\n",
    "- **Input Sentence**: \"The monkey ate the banana because it was too hungry.\"\n",
    "- **Self-Attention Calculation**:\n",
    "  - For the word \"it,\" calculate the dot product of its query vector with the key vectors of all other words.\n",
    "  - Apply the softmax function to obtain attention scores.\n",
    "  - Multiply each value vector by its attention score and sum them to get the final representation for \"it.\"\n",
    "\n",
    "Self-attention enables transformers to effectively capture the relationships between words, enhancing their ability to understand and generate natural language. This mechanism is a key reason why transformers have become so powerful in various NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **12. Multi-Head Attention and Feedforward Network in Transformers**\n",
    "\n",
    "Multi-head attention is an extension of the self-attention mechanism that allows the model to focus on different parts of the input sequence simultaneously. Here's a detailed explanation:\n",
    "\n",
    "1. **Multi-Head Attention**:\n",
    "   - **Concept**: Instead of having a single self-attention mechanism, multi-head attention uses multiple self-attention mechanisms (heads) in parallel.\n",
    "   - **Purpose**: Each head can learn different aspects of the input data. For example, one head might focus on the relationship between nouns and adjectives, while another might connect pronouns to their subjects.\n",
    "\n",
    "2. **Mechanism**:\n",
    "   - **Inputs**: Each multi-head attention block receives three inputs: query ($Q$), key ($K$), and value ($V$).\n",
    "   - **Linear Layers**: The query, key, and value vectors are passed through separate fully connected linear layers for each attention head.\n",
    "   - **Parallel Attention Heads**: The model computes the self-attention for each head in parallel.\n",
    "   - **Concatenation and Linear Transformation**: The outputs of all attention heads are concatenated and passed through another linear layer to produce the final output.\n",
    "\n",
    "3. **Formulas**:\n",
    "   - **Scaled Dot-Product Attention** (for each head):\n",
    "     $$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V $$\n",
    "   - **Multi-Head Attention**:\n",
    "     $$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W^O $$\n",
    "     - Where each head is computed as:\n",
    "       $$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "     - $W_i^Q$, $W_i^K$, $W_i^V$: Weight matrices for the $i$-th head.\n",
    "     - $W^O$: Output weight matrix.\n",
    "\n",
    "4. **Benefits**:\n",
    "   - **Diverse Representations**: By having multiple heads, the model can capture different types of relationships and dependencies in the data.\n",
    "   - **Joint Attention**: The model can jointly attend to information from different representation subspaces and at different positions.\n",
    "\n",
    "5. **Feedforward Network**:\n",
    "   - **Position-Wise Feedforward Layers**: After the multi-head attention layer, the output is passed through a feedforward neural network.\n",
    "   - **Structure**: Typically consists of two linear transformations with a ReLU activation in between.\n",
    "     $$ \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2 $$\n",
    "     - $W_1$, $W_2$: Weight matrices.\n",
    "     - $b_1$, $b_2$: Bias terms.\n",
    "\n",
    "6. **BERT's Multi-Head Attention**:\n",
    "   - BERT uses 12 attention heads in each of its layers.\n",
    "   - This allows BERT to focus on multiple aspects of the input simultaneously, making richer connections between words.\n",
    "\n",
    "**Example Workflow**:\n",
    "- **Input Sentence**: \"The monkey ate the banana because it was too hungry.\"\n",
    "- **Multi-Head Attention**: Each head processes the sentence to capture different relationships (e.g., \"monkey\" with \"it\", \"banana\" with \"ate\").\n",
    "- **Feedforward Network**: The combined output from the attention heads is further processed to enhance the model's understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **13. BERT and Text Classification**\n",
    "\n",
    "Fine-tuning a pre-trained BERT model for text classification involves several steps. Here's a detailed overview using the IMDB dataset as an example:\n",
    "\n",
    "1. **Dataset**:\n",
    "   - **IMDB Dataset**: Contains movie reviews with two columns:\n",
    "     - **Text Column**: The review text.\n",
    "     - **Label Column**: Indicates the sentiment of the review (1 for positive, 0 for negative).\n",
    "\n",
    "2. **Pre-training Step**:\n",
    "   - During pre-training, BERT was trained with tasks like next sentence prediction, which is a form of text classification.\n",
    "   - A linear layer was added at the end of the BERT model, using the embedding from the `[CLS]` token.\n",
    "\n",
    "3. **Fine-Tuning for Text Classification**:\n",
    "   - **Linear Classifier**: Add a linear classifier layer to the pre-trained BERT model.\n",
    "   - **Dropout Layer**: Often added to reduce overfitting.\n",
    "   - **Input**: Use the final embedding of the `[CLS]` token as the input to the linear classifier.\n",
    "\n",
    "4. **Training**:\n",
    "   - **Label Dataset**: Train the model with labeled data (e.g., movie reviews and their associated sentiments).\n",
    "   - **Process**: The model learns to classify the sentiment of the reviews based on the training data.\n",
    "\n",
    "5. **Hidden States**:\n",
    "   - The final embeddings in the hidden state are not used for the classification task but capture increasingly enhanced embeddings.\n",
    "   - These embeddings are useful for other tasks like named entity recognition or question answering.\n",
    "\n",
    "**Example Workflow**:\n",
    "- **Step 1**: Load the IMDB dataset.\n",
    "- **Step 2**: Tokenize the text using BERT's tokenizer.\n",
    "- **Step 3**: Add a linear classifier and dropout layer to the pre-trained BERT model.\n",
    "- **Step 4**: Train the model on the labeled dataset.\n",
    "- **Step 5**: Evaluate the model's performance on a test set.\n",
    "\n",
    "By fine-tuning BERT in this manner, it can effectively classify text based on the learned representations from the pre-training phase, making it a powerful tool for sentiment analysis and other text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Model Fine-Tuning  \n",
    "By following these steps and leveraging the Datasets library, you can fine-tune a BERT model for text classification using the IMDB dataset, ensuring efficient data handling and robust model performance. If you have any questions or need further assistance, feel free to ask!\n",
    "\n",
    "```python\n",
    "# Install the required libraries\n",
    "!pip install transformers datasets\n",
    "\n",
    "# Import the necessary libraries\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "# Load the IMDB dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "checkpoint = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the text and pad/truncate to a maximum length\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# Apply the tokenization function to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Load the pre-trained BERT model with a classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',            # Directory to save the model checkpoints\n",
    "    evaluation_strategy='epoch',       # Evaluate the model at the end of each epoch\n",
    "    learning_rate=2e-5,                # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=8,     # Batch size for training\n",
    "    per_device_eval_batch_size=8,      # Batch size for evaluation\n",
    "    num_train_epochs=3,                # Number of training epochs\n",
    "    weight_decay=0.01,                 # Weight decay for regularization\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                       # The pre-trained BERT model\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=tokenized_datasets['train'],  # Training dataset\n",
    "    eval_dataset=tokenized_datasets['test'],    # Evaluation dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "```\n",
    "\n",
    "### Using the Datasets Library\n",
    "\n",
    "The Datasets library by Hugging Face makes it easy to access and manage datasets for NLP tasks. Here's how to use it with the IMDB dataset:\n",
    "\n",
    "1. **Install the Library**:\n",
    "   ```python\n",
    "   # Install the datasets library\n",
    "   !pip install datasets\n",
    "   ```\n",
    "\n",
    "2. **Load the Dataset**:\n",
    "   ```python\n",
    "   # Import the load_dataset function\n",
    "   from datasets import load_dataset\n",
    "\n",
    "   # Load the IMDB dataset\n",
    "   dataset = load_dataset('imdb')\n",
    "   ```\n",
    "\n",
    "3. **Explore the Dataset**:\n",
    "   ```python\n",
    "   # Check the dataset structure\n",
    "   print(dataset)\n",
    "\n",
    "   # Access the first entry in the training set\n",
    "   print(dataset['train'][0])\n",
    "   ```\n",
    "\n",
    "4. **Reduce Dataset Size for Quick Training**:\n",
    "   ```python\n",
    "   # Reduce the dataset size for quick training\n",
    "   small_train_dataset = dataset['train'].shuffle(seed=42).select([i for i in list(range(2000))])\n",
    "   small_test_dataset = dataset['test'].shuffle(seed=42).select([i for i in list(range(400))])\n",
    "\n",
    "   # Create validation split\n",
    "   small_train_dataset = small_train_dataset.train_test_split(test_size=0.2)\n",
    "   small_train_dataset['validation'] = small_train_dataset.pop('test')\n",
    "\n",
    "   # Update the dataset dictionary\n",
    "   dataset = {\n",
    "       'train': small_train_dataset['train'],\n",
    "       'validation': small_train_dataset['validation'],\n",
    "       'test': small_test_dataset\n",
    "   }\n",
    "   ```\n",
    "\n",
    "5. **Delete Unsupervised Split**:\n",
    "   ```python\n",
    "   # Remove the unsupervised split\n",
    "   if 'unsupervised' in dataset:\n",
    "       del dataset['unsupervised']\n",
    "   ```\n",
    "\n",
    "### Additional Information\n",
    "\n",
    "**Pre-training and Fine-tuning**:\n",
    "- **Pre-training**: BERT was pre-trained on large corpora like Wikipedia and BookCorpus using tasks such as Masked Language Modeling (MLM) and Next Sentence Prediction (NSP). This helps BERT understand language patterns and context.\n",
    "- **Fine-tuning**: For specific tasks like text classification, BERT is fine-tuned on labeled datasets. The `[CLS]` token's final embedding is used as input to a linear classifier, which is trained to predict the class labels.\n",
    "\n",
    "**Model Architecture**:\n",
    "- **BERT Base**: Consists of 12 layers (transformer blocks), 768 hidden units, and 12 attention heads.\n",
    "- **BERT Large**: Consists of 24 layers, 1024 hidden units, and 16 attention heads.\n",
    "\n",
    "**Handling Imbalanced Data**:\n",
    "- **Class Weights**: Adjust the loss function to account for class imbalance by assigning higher weights to minority classes.\n",
    "- **Data Augmentation**: Generate synthetic examples for underrepresented classes to balance the dataset.\n",
    "\n",
    "**Evaluation Metrics**:\n",
    "- **Accuracy**: Measures the proportion of correctly predicted instances.\n",
    "- **Precision, Recall, F1-Score**: Useful for imbalanced datasets to evaluate the model's performance on each class."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
